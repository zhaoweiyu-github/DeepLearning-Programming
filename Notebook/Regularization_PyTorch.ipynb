{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import PIL\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reference: [design a shallow neural network for image classification](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Network layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=3, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 3)\n",
    "        self.dropout = nn.Dropout(p = 0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        x = self.dropout(x) # added dropout\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr = learning_rate, weight_decay = 0.01) # weight decay: L2 penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    'Characterizes an image dataset for PyTorch'\n",
    "    def __init__(self, list_files, labels, transform = None):\n",
    "        'Initialization'\n",
    "        self.labels = labels\n",
    "        self.list_files = list_files\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.list_files)\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Select sample\n",
    "        file_dir = self.list_files[index]\n",
    "\n",
    "        # Load data and get label\n",
    "        image = PIL.Image.open(file_dir)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        X = image\n",
    "        y = torch.tensor(self.labels[file_dir])\n",
    "\n",
    "        return X, y\n",
    "\n",
    "data_dir = \"/mnt/Storage/home/yuzhaowei/.keras/datasets/flower_photos\"\n",
    "labels = {}\n",
    "daisy_images = [os.path.join((data_dir), \"daisy\") + \"/\" + i for i in os.listdir(os.path.join((data_dir), \"daisy\"))]\n",
    "rose_images = [os.path.join((data_dir), \"roses\") + \"/\" + i for i in os.listdir(os.path.join((data_dir), \"roses\"))]\n",
    "sunflowers_images = [os.path.join((data_dir), \"sunflowers\") + \"/\" + i for i in os.listdir(os.path.join((data_dir), \"sunflowers\"))]\n",
    "samples = np.concatenate([np.random.choice(daisy_images, 500), np.random.choice(rose_images, 500), np.random.choice(sunflowers_images, 500)])\n",
    "\n",
    "for image in daisy_images:\n",
    "    labels[image] = 0 # dasiy\n",
    "for image in rose_images:\n",
    "    labels[image] = 1 # roses\n",
    "for image in sunflowers_images:\n",
    "    labels[image] = 2 # sunflowers\n",
    "\n",
    "class Rescale():\n",
    "    \"\"\"Reize image in a sample to a given size\"\"\"\n",
    "    def __call__(self, image):\n",
    "        image_resize = np.array(T.Resize(size = (32, 32))(image))\n",
    "        return(image_resize)\n",
    "    \n",
    "# Converts a PIL Image or numpy.ndarray (H x W x C) in the range [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0] if the PIL Image belongs to one of the modes (L, LA, P, I, F, RGB, YCbCr, RGBA, CMYK, 1) or if the numpy.ndarray has dtype = np.uint8\n",
    "composed = T.Compose([Rescale(), T.ToTensor()]) # composed two treatments for input images    \n",
    "        \n",
    "    \n",
    "dataset = Dataset(samples, labels, transform = composed)\n",
    "train_set, test_set = torch.utils.data.random_split(dataset, lengths = [int(1500 * 0.8), int(1500 * 0.2)])\n",
    "\n",
    "batch_size = 120\n",
    "n_iters = 100\n",
    "num_epochs = n_iters / (len(train_set) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "print(num_epochs)\n",
    "\n",
    "params = {'batch_size': batch_size,\n",
    "          'shuffle': True,\n",
    "          'num_workers': 6,\n",
    "         'drop_last' : True # set to True to drop the last incomplete batch, if the dataset size is not divisible by the batch size.\n",
    "         }\n",
    "train_generator = torch.utils.data.DataLoader(train_set, **params)\n",
    "test_generator = torch.utils.data.DataLoader(test_set, **params)\n",
    "\n",
    "import collections\n",
    "isinstance(train_generator, collections.Iterable)\n",
    "isinstance(test_generator, collections.Iterable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([120, 3, 32, 32])\n",
      "torch.Size([120, 3, 32, 32])\n",
      "torch.Size([120, 3, 32, 32])\n",
      "torch.Size([120, 3, 32, 32])\n",
      "torch.Size([120, 3, 32, 32])\n",
      "torch.Size([120, 3, 32, 32])\n",
      "torch.Size([120, 3, 32, 32])\n",
      "torch.Size([120, 3, 32, 32])\n",
      "torch.Size([120, 3, 32, 32])\n",
      "torch.Size([120, 3, 32, 32])\n",
      "Iteration: 10. Loss: 1.1063742637634277. Accuracy: 40.83333206176758\n",
      "torch.Size([120, 3, 32, 32])\n",
      "torch.Size([120, 3, 32, 32])\n",
      "torch.Size([120, 3, 32, 32])\n",
      "torch.Size([120, 3, 32, 32])\n",
      "torch.Size([120, 3, 32, 32])\n",
      "torch.Size([120, 3, 32, 32])\n",
      "torch.Size([120, 3, 32, 32])\n",
      "torch.Size([120, 3, 32, 32])\n",
      "torch.Size([120, 3, 32, 32])\n",
      "torch.Size([120, 3, 32, 32])\n",
      "Iteration: 20. Loss: 1.105272889137268. Accuracy: 40.0\n",
      "torch.Size([120, 3, 32, 32])\n",
      "torch.Size([120, 3, 32, 32])\n",
      "torch.Size([120, 3, 32, 32])\n",
      "torch.Size([120, 3, 32, 32])\n",
      "torch.Size([120, 3, 32, 32])\n",
      "torch.Size([120, 3, 32, 32])\n",
      "torch.Size([120, 3, 32, 32])\n",
      "torch.Size([120, 3, 32, 32])\n",
      "torch.Size([120, 3, 32, 32])\n",
      "torch.Size([120, 3, 32, 32])\n",
      "Iteration: 30. Loss: 1.1097767353057861. Accuracy: 40.83333206176758\n",
      "torch.Size([120, 3, 32, 32])\n",
      "torch.Size([120, 3, 32, 32])\n",
      "torch.Size([120, 3, 32, 32])\n",
      "torch.Size([120, 3, 32, 32])\n",
      "torch.Size([120, 3, 32, 32])\n",
      "torch.Size([120, 3, 32, 32])\n",
      "torch.Size([120, 3, 32, 32])\n",
      "torch.Size([120, 3, 32, 32])\n",
      "torch.Size([120, 3, 32, 32])\n",
      "torch.Size([120, 3, 32, 32])\n",
      "Iteration: 40. Loss: 1.0968239307403564. Accuracy: 39.16666793823242\n",
      "torch.Size([120, 3, 32, 32])\n",
      "torch.Size([120, 3, 32, 32])\n",
      "torch.Size([120, 3, 32, 32])\n",
      "torch.Size([120, 3, 32, 32])\n",
      "torch.Size([120, 3, 32, 32])\n",
      "torch.Size([120, 3, 32, 32])\n",
      "torch.Size([120, 3, 32, 32])\n",
      "torch.Size([120, 3, 32, 32])\n",
      "torch.Size([120, 3, 32, 32])\n",
      "torch.Size([120, 3, 32, 32])\n",
      "Iteration: 50. Loss: 1.0985528230667114. Accuracy: 42.08333206176758\n",
      "torch.Size([120, 3, 32, 32])\n",
      "torch.Size([120, 3, 32, 32])\n",
      "torch.Size([120, 3, 32, 32])\n",
      "torch.Size([120, 3, 32, 32])\n",
      "torch.Size([120, 3, 32, 32])\n",
      "torch.Size([120, 3, 32, 32])\n",
      "torch.Size([120, 3, 32, 32])\n",
      "torch.Size([120, 3, 32, 32])\n",
      "torch.Size([120, 3, 32, 32])\n",
      "torch.Size([120, 3, 32, 32])\n",
      "Iteration: 60. Loss: 1.1034022569656372. Accuracy: 40.0\n",
      "torch.Size([120, 3, 32, 32])\n",
      "torch.Size([120, 3, 32, 32])\n",
      "torch.Size([120, 3, 32, 32])\n",
      "torch.Size([120, 3, 32, 32])\n",
      "torch.Size([120, 3, 32, 32])\n",
      "torch.Size([120, 3, 32, 32])\n",
      "torch.Size([120, 3, 32, 32])\n",
      "torch.Size([120, 3, 32, 32])\n",
      "torch.Size([120, 3, 32, 32])\n",
      "torch.Size([120, 3, 32, 32])\n",
      "Iteration: 70. Loss: 1.104246735572815. Accuracy: 39.16666793823242\n",
      "torch.Size([120, 3, 32, 32])\n",
      "torch.Size([120, 3, 32, 32])\n",
      "torch.Size([120, 3, 32, 32])\n",
      "torch.Size([120, 3, 32, 32])\n",
      "torch.Size([120, 3, 32, 32])\n",
      "torch.Size([120, 3, 32, 32])\n",
      "torch.Size([120, 3, 32, 32])\n",
      "torch.Size([120, 3, 32, 32])\n",
      "torch.Size([120, 3, 32, 32])\n",
      "torch.Size([120, 3, 32, 32])\n",
      "Iteration: 80. Loss: 1.1035728454589844. Accuracy: 38.75\n",
      "torch.Size([120, 3, 32, 32])\n",
      "torch.Size([120, 3, 32, 32])\n",
      "torch.Size([120, 3, 32, 32])\n",
      "torch.Size([120, 3, 32, 32])\n",
      "torch.Size([120, 3, 32, 32])\n",
      "torch.Size([120, 3, 32, 32])\n",
      "torch.Size([120, 3, 32, 32])\n",
      "torch.Size([120, 3, 32, 32])\n",
      "torch.Size([120, 3, 32, 32])\n",
      "torch.Size([120, 3, 32, 32])\n",
      "Iteration: 90. Loss: 1.103756070137024. Accuracy: 37.91666793823242\n",
      "torch.Size([120, 3, 32, 32])\n",
      "torch.Size([120, 3, 32, 32])\n",
      "torch.Size([120, 3, 32, 32])\n",
      "torch.Size([120, 3, 32, 32])\n",
      "torch.Size([120, 3, 32, 32])\n",
      "torch.Size([120, 3, 32, 32])\n",
      "torch.Size([120, 3, 32, 32])\n",
      "torch.Size([120, 3, 32, 32])\n",
      "torch.Size([120, 3, 32, 32])\n",
      "torch.Size([120, 3, 32, 32])\n",
      "Iteration: 100. Loss: 1.099623203277588. Accuracy: 41.25\n"
     ]
    }
   ],
   "source": [
    "iter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_generator):\n",
    "        # Load images as Variable\n",
    "        # images = images.requires_grad_()\n",
    "        images = images.requires_grad_()\n",
    "        print(images.shape)\n",
    "        labels = labels\n",
    "        # print(labels)\n",
    "\n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass to get output/logits\n",
    "        outputs = net(images)\n",
    "        # print(outputs)\n",
    "\n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        iter += 1\n",
    "\n",
    "        if iter % 10 == 0:\n",
    "            # Calculate Accuracy         \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterate through test dataset\n",
    "            for images, labels in test_generator:\n",
    "                # Load images to a Torch Variable\n",
    "                # images = images.view(-1, 3*224*224).requires_grad_()\n",
    "\n",
    "                # Forward pass only to get logits/output\n",
    "                outputs = net(images)\n",
    "\n",
    "                # Get predictions from the maximum value\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "                # Total number of labels\n",
    "                total += labels.size(0)\n",
    "\n",
    "                # Total correct predictions\n",
    "                correct += (predicted == labels).sum()\n",
    "\n",
    "            accuracy = 100 * correct / total\n",
    "\n",
    "            # Print Loss\n",
    "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
